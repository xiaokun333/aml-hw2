{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddd8a3e",
   "metadata": {},
   "source": [
    "# Homework 2: Linear Models for Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019346dd",
   "metadata": {},
   "source": [
    "Due 10/21 at 11:59pm\n",
    "\n",
    "In this notebook, we will be implementing three linear models: linear regression, logistic regression, and SVM. We will see that despite some of their differences at the surface, these linear models (and many machine learning models in general) are fundamentally doing the same thing - that is, optimizing model parameters to minimize a loss function on data.\n",
    "\n",
    "**Note: There are two notebooks in Homework 2. Please also complete the other notebook `HW2_Decision_Trees.ipynb` for full credit on this assignment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d934a1",
   "metadata": {},
   "source": [
    "![comic](https://pbs.twimg.com/media/ESlslPWWkAAcNP-?format=jpg&name=900x900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036015f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:06:13.650605Z",
     "start_time": "2021-10-07T03:06:10.384483Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78021b1",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c7f2e",
   "metadata": {},
   "source": [
    "### 1.1 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f9f3f",
   "metadata": {},
   "source": [
    "In part 1, we will use two datasets to train and evaluate our linear regression model. \n",
    "\n",
    "The first dataset will be a synthetic dataset sampled from the following equations:\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\text{Normal}(0, 3) \\\\\n",
    "y = 3x + 10 + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372e874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:06:57.968118Z",
     "start_time": "2021-10-07T03:06:57.952242Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "epsilon = np.random.normal(0, 3, 100)   # Sample epsilon from a Normal distribution\n",
    "x = np.linspace(0, 10, 100)             # Sample x evenly between 0 and 100\n",
    "y = 3 * x + 10 + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154bf937",
   "metadata": {},
   "source": [
    "Note that the above dataset satisfies all the assumptions of a linear regression model:\n",
    "- Linearity: $y$ is a linear (technically affine) function of $x$.\n",
    "- Independence: the $x$'s are independently drawn, and not dependent on each other.\n",
    "- Homoscedasticity: the $\\epsilon$'s, and thus the $y$'s, have constant variance.\n",
    "- Normality: the $\\epsilon$'s are drawn from a Normal distribution (i.e. Normally-distributed errors)\n",
    "\n",
    "These properties, as well as the simplicity of this dataset, will make it a good test case to check if our linear regression model is working properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb3ad2",
   "metadata": {},
   "source": [
    "**Plot y vs x in the synthetic dataset as a scatter plot. Label your axes and make sure your y-axis starts from 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423abe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:00.232041Z",
     "start_time": "2021-10-07T03:06:59.916995Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694ae54",
   "metadata": {},
   "source": [
    "The second dataset we will be using is an [auto MPG dataset](https://archive.ics.uci.edu/ml/datasets/Auto+MPG). This dataset contains various characteristics for around 400 cars. We will use linear regression to predict the mpg label from seven features (4 continuous, 3 discrete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f2af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:02.596598Z",
     "start_time": "2021-10-07T03:07:02.535349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load auto MPG dataset\n",
    "auto_mpg_df = pd.read_csv('auto-mpg.csv')\n",
    "\n",
    "# drop some rows with missing entries\n",
    "auto_mpg_df = auto_mpg_df[auto_mpg_df['horsepower'] != '?']\n",
    "# Cast horsepower column to float\n",
    "auto_mpg_df['horsepower'] = auto_mpg_df['horsepower'].astype(float)\n",
    "\n",
    "auto_mpg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f145c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:05.082781Z",
     "start_time": "2021-10-07T03:07:05.069892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "auto_mpg_X = auto_mpg_df.drop(columns=['mpg'])\n",
    "auto_mpg_y = auto_mpg_df['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd151043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T17:31:07.080198Z",
     "start_time": "2021-10-04T17:31:07.069578Z"
    }
   },
   "source": [
    "**Plot the distribution of the label (mpg) using a histogram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b0bc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:06.332359Z",
     "start_time": "2021-10-07T03:07:06.093493Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf519fc3",
   "metadata": {},
   "source": [
    "**Plot the relationships between the label (mpg) and the continuous features (displacement, horsepower, weight, acceleration) using a small multiple of scatter plots. Make sure to label the axes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a1e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:09.323242Z",
     "start_time": "2021-10-07T03:07:08.798221Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a790911",
   "metadata": {},
   "source": [
    "**Plot the relationships between the label (mpg) and the discrete features (cylinders, model year, origin) using a small multiple of box plots. Make sure to label the axes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b3af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:13.376980Z",
     "start_time": "2021-10-07T03:07:12.606215Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeed2e7",
   "metadata": {},
   "source": [
    "**From the visualizations above, do you think linear regression is a good model for this problem? Why and/or why not?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3abee55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T19:32:33.476849Z",
     "start_time": "2021-10-04T19:32:33.470930Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e30aa",
   "metadata": {},
   "source": [
    "### 1.2 Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a60a5d",
   "metadata": {},
   "source": [
    "Before we can fit a linear regression model, there are several pre-processing steps we should apply to the datasets:\n",
    "1. Encode categorial features appropriately.\n",
    "2. Split the dataset into training (60%), validation (20%), and test (20%) sets.\n",
    "3. Standardize the columns in the feature matrices X_train, X_val, and X_test to have zero mean and unit variance. To avoid information leakage, learn the standardization parameters (mean, variance) from X_train, and apply it to X_train, X_val, and X_test.\n",
    "4. Add a column of ones to the feature matrices X_train, X_val, and X_test. This is a common trick so that we can learn a coefficient for the bias term of a linear model.\n",
    "\n",
    "The processing steps on the synthetic dataset have been provided for you below as a reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80adc88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:23.433495Z",
     "start_time": "2021-10-07T03:07:23.412008Z"
    }
   },
   "outputs": [],
   "source": [
    "X = x.reshape((100, 1))   # Turn the x vector into a feature matrix X\n",
    "\n",
    "# 1. No categorical features in the synthetic dataset (skip this step)\n",
    "\n",
    "# 2. Split the dataset into training (60%), validation (20%), and test (20%) sets\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25, random_state=0)\n",
    "\n",
    "# 3. Standardize the columns in the feature matrices\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)   # Fit and transform scalar on X_train\n",
    "X_val = scaler.transform(X_val)           # Transform X_val\n",
    "X_test = scaler.transform(X_test)         # Transform X_test\n",
    "\n",
    "# 4. Add a column of ones to the feature matrices\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_val = np.hstack([np.ones((X_val.shape[0], 1)), X_val])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "print(X_train[:5], '\\n\\n', y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887e6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:25.356388Z",
     "start_time": "2021-10-07T03:07:25.349130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify that columns (other than the ones column) have 0 mean, 1 variance\n",
    "print(X_train.mean(axis=0), X_train.std(axis=0))\n",
    "print(X_val.mean(axis=0), X_val.std(axis=0))\n",
    "print(X_test.mean(axis=0), X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625db1b",
   "metadata": {},
   "source": [
    "**Now, apply the same processing steps on the auto MPG dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac8606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:28.205210Z",
     "start_time": "2021-10-07T03:07:28.172240Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69117ed9",
   "metadata": {},
   "source": [
    "At the end of this pre-processing, you should have the following vectors and matrices: \n",
    "- Syntheic dataset: X_train, X_val, X_test, y_train, y_val, y_test\n",
    "- Auto MPG dataset: auto_mpg_X_train, auto_mpg_X_val, auto_mpg_X_test, auto_mpg_y_train, auto_mpg_y_val, auto_mpg_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7350d4c",
   "metadata": {},
   "source": [
    "### 1.3 Implement Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765ae70",
   "metadata": {},
   "source": [
    "Now, we can implement our linear regression model! Specifically, we will be implementing ridge regression, which is linear regression with L2 regularization. Given an (m x n) feature matrix $X$, an (m x 1) label vector $y$, and an (n x 1) weight vector $w$, the hypothesis function for linear regression is:\n",
    "\n",
    "$$\n",
    "y = X w\n",
    "$$\n",
    "\n",
    "Note that we can omit the bias term here because we have included a column of ones in our $X$ matrix, so the bias term is learned implicitly as a part of $w$. This will make our implementation easier.\n",
    "\n",
    "Our objective in linear regression is to learn the weights $w$ which best fit the data. This notion can be formalized as finding the optimal $w$ which minimizes the following loss function:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\| X w - y \\|^2_2 + \\alpha \\| w \\|^2_2 \\\\\n",
    "$$\n",
    "\n",
    "This is the ridge regression loss function. The $\\| X w - y \\|^2_2$ term penalizes predictions $Xw$ which are not close to the label $y$. And the $\\alpha \\| w \\|^2_2$ penalizes large weight values, to favor a simpler, more generalizable model. The $\\alpha$ hyperparameter, known as the regularization parameter, is used to tune the complexity of the model - a higher $\\alpha$ results in smaller weights and lower complexity, and vice versa. Setting $\\alpha = 0$ gives us vanilla linear regression.\n",
    "\n",
    "Conveniently, ridge regression has a closed-form solution which gives us the optimal $w$ without having to do iterative methods such as gradient descent. The closed-form solution, known as the Normal Equations, is given by:\n",
    "\n",
    "$$\n",
    "w = (X^T X + \\alpha I)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fbd72",
   "metadata": {},
   "source": [
    "**Implement a `LinearRegression` class with two methods: `train` and `predict`.** You may NOT use sklearn for this implementation. You may, however, use `np.linalg.solve` to find the closed-form solution. It is highly recommended that you vectorize your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e97b1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:46.761054Z",
     "start_time": "2021-10-07T03:07:46.747099Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    '''\n",
    "    Linear regression model with L2-regularization (i.e. ridge regression).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    alpha: regularization parameter\n",
    "    w: (n x 1) weight vector\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, alpha=0):\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''Trains model using ridge regression closed-form solution \n",
    "        (sets w to its optimal value).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        ### Your code here\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''Predicts on X using trained model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: (m x 1) prediction vector\n",
    "        '''\n",
    "        ### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be4e8e",
   "metadata": {},
   "source": [
    "### 1.4 Train,  Evaluate, and Interpret Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588fb2ca",
   "metadata": {},
   "source": [
    "**Using your `LinearRegression` implementation above, train a vanilla linear regression model ($\\alpha = 0$) on (X_train, y_train) from the synthetic dataset. Use this trained model to predict on X_test. Report the first 5 predictions on X_test, along with the actual labels in y_test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730c659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:07:55.354848Z",
     "start_time": "2021-10-07T03:07:55.340409Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede54892",
   "metadata": {},
   "source": [
    "**Plot a scatter plot of y_test vs X_test (just the non-ones column). Then, using the weights from the trained model above, plot the best-fit line for this data on the same figure.** If your line goes through the data points, you have likely implemented the linear regression correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175056e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:08:08.526405Z",
     "start_time": "2021-10-07T03:08:08.349406Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53bf8ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T21:00:53.468542Z",
     "start_time": "2021-10-04T21:00:53.450204Z"
    }
   },
   "source": [
    "**Train a linear regression model ($\\alpha = 0$) on the auto MPG training data. Make predictions and report the mean-squared error (MSE) on the training, validation, and test sets. Report the first 5 predictions on the test set, along with the actual labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a1b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:08:23.174084Z",
     "start_time": "2021-10-07T03:08:23.145458Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c5951",
   "metadata": {},
   "source": [
    "**As a baseline model, use the mean of the training labels (auto_mpg_y_train) as the prediction for all instances. Report the mean-squared error (MSE) on the training, validation, and test sets using this baseline.** This is a common baseline used in regression problems and tells you if your model is any good. Your linear regression MSEs should be much lower than these baseline MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89320ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:08:29.346144Z",
     "start_time": "2021-10-07T03:08:29.338157Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdc212",
   "metadata": {},
   "source": [
    "**Interpret your model trained on the auto MPG dataset using a bar chart of the model weights.** Make sure to label the bars (x-axis) and don't forget the bias term! Use lecture 3, slide 15 as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f4aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:08:32.027517Z",
     "start_time": "2021-10-07T03:08:31.816682Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e9bc9",
   "metadata": {},
   "source": [
    "**According to your model, which features are the greatest contributors to the MPG?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fba43d",
   "metadata": {},
   "source": [
    "### 1.5 Tune Regularization Parameter $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342b9f7",
   "metadata": {},
   "source": [
    "Now, let's do ridge regression and tune the $\\alpha$ regularization parameter on the auto MPG dataset.\n",
    "\n",
    "**Sweep out values for $\\alpha$ using `alphas = np.logspace(-2, 1, 10)`. Perform a grid search over these $\\alpha$ values, recording the training and validation MSEs for each $\\alpha$. A simple grid search is fine, no need for k-fold cross validation. Plot the training and validation MSEs as a function of $\\alpha$ on a single figure. Make sure to label the axes and the training and validation MSE curves. Use a log scale for the x-axis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5cdc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:08:59.874380Z",
     "start_time": "2021-10-07T03:08:59.127671Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d0a77",
   "metadata": {},
   "source": [
    "**Explain your plot above. How do training and validation MSE behave with decreasing model complexity (increasing $\\alpha$)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678f04d",
   "metadata": {},
   "source": [
    "**Using the $\\alpha$ which gave the best validation MSE above, train a model on the training set. Report the value of $\\alpha$ and its training, validation, and test MSE.** This is the final tuned model which you would deploy in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef85470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:09:10.873423Z",
     "start_time": "2021-10-07T03:09:10.853420Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e4ef3",
   "metadata": {},
   "source": [
    "## Part 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee183cee",
   "metadata": {},
   "source": [
    "### 2.1 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc056afe",
   "metadata": {},
   "source": [
    "<img src=\"https://pathology.jhu.edu/build/assets/breast/_image1200/368/ILC.jpg\" alt=\"cells\" width=\"400\"/>\n",
    "\n",
    "In parts 2 and 3, we will be using a [breast cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) for classification. Given 30 continuous features describing the nuclei of cells in a digitized image of a [fine needle aspirate (FNA)](https://www.cancer.org/cancer/breast-cancer/screening-tests-and-early-detection/breast-biopsy/fine-needle-aspiration-biopsy-of-the-breast.html) of a breast mass, we will train logistic regression and SVM models to classify each sample as benign (B) or malignant (M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9122604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:09:25.362515Z",
     "start_time": "2021-10-07T03:09:25.312574Z"
    }
   },
   "outputs": [],
   "source": [
    "cancer_df = pd.read_csv('breast-cancer.csv')\n",
    "cancer_df = cancer_df.drop(columns=['id', 'Unnamed: 32'])\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc47d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:09:27.715212Z",
     "start_time": "2021-10-07T03:09:27.710656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "cancer_X = cancer_df.drop(columns=['diagnosis'])\n",
    "cancer_y = cancer_df['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439ea6e",
   "metadata": {},
   "source": [
    "**Compute the distribution of the labels. What is the probability of observing the majority class?** This is a common baseline for accuracy in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e7011",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:09:30.967598Z",
     "start_time": "2021-10-07T03:09:30.949976Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95068d90",
   "metadata": {},
   "source": [
    "**Plot the relationships between the label (diagnosis) and the 30 features using a small multiple of box plots. Make sure to label the axes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fafe86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:09:52.542614Z",
     "start_time": "2021-10-07T03:09:48.629764Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e9b05",
   "metadata": {},
   "source": [
    "### 2.2 Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8628d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T20:13:38.706095Z",
     "start_time": "2021-10-06T20:13:38.680051Z"
    }
   },
   "source": [
    "**Apply the following pre-processing steps to the breast cancer dataset:**\n",
    "\n",
    "1. Encode the categorical label as 0 (B) or 1 (M).\n",
    "2. Convert the label from a Pandas series to a Numpy (m x 1) vector. If you don't do this, it may cause problems when implementing the logistic regression model (certain broadcasting operations may fail unexpectedly).\n",
    "2. Split the dataset into training (60%), validation (20%), and test (20%) sets.\n",
    "3. Standardize the columns in the feature matrices cancer_X_train, cancer_X_val, and cancer_X_test to have zero mean and unit variance. To avoid information leakage, learn the standardization parameters (mean, variance) from cancer_X_train, and apply it to cancer_X_train, cancer_X_val, and cancer_X_test.\n",
    "4. Add a column of ones to the feature matrices cancer_X_train, cancer_X_val, and cancer_X_test. This is a common trick so that we can learn a coefficient for the bias term of a linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1011ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:10:29.217123Z",
     "start_time": "2021-10-07T03:10:29.196688Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e72f5",
   "metadata": {},
   "source": [
    "At the end of this pre-processing, you should have the following vectors and matrices: cancer_X_train, cancer_X_val, cancer_X_test, cancer_y_train, cancer_y_val, cancer_y_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a80d0",
   "metadata": {},
   "source": [
    "### 2.3 Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951bf9f",
   "metadata": {},
   "source": [
    "We will now implement logistic regression with L2 regularization. Given an (m x n) feature matrix $X$, an (m x 1) label vector $y$, and an (n x 1) weight vector $w$, the hypothesis function for logistic regression is:\n",
    "\n",
    "$$\n",
    "y = \\sigma(X w)\n",
    "$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, i.e. the sigmoid function. This function scales the prediction to be a probability between 0 and 1, and can then be thresholded to get a discrete class prediction.\n",
    "\n",
    "Just as with linear regression, our objective in logistic regression is to learn the weights $ùë§$ which best fit the data. For L2-regularized logistic regression, we find an optimal $w$ to minimize the following loss function:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\ -y^T \\ \\text{log}(\\sigma(Xw)) \\ - \\  (\\mathbf{1} - y)^T \\ \\text{log}(\\mathbf{1} - \\sigma(Xw)) \\ + \\ \\alpha \\| w \\|^2_2 \\\\\n",
    "$$\n",
    "\n",
    "Unlike linear regression, however, logistic regression has no closed-form solution for the optimal $w$. So, we will use gradient descent to find the optimal $w$. The (n x 1) gradient vector $g$ for the loss function above is:\n",
    "\n",
    "$$\n",
    "g = X^T \\Big(\\sigma(Xw) - y\\Big) + 2 \\alpha w\n",
    "$$\n",
    "\n",
    "Below is pseudocode for gradient descent to find the optimal $w$. You should first initialize $w$ (e.g. to a (n x 1) zero vector). Then, for some number of epochs $t$, you should update $w$ with $w - \\eta g $, where $\\eta$ is the learning rate and $g$ is the gradient. You can learn more about gradient descent [here](https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM).\n",
    "\n",
    "> $w = \\mathbf{0}$\n",
    "> \n",
    "> $\\text{for } i = 1, 2, ..., t$\n",
    ">\n",
    "> $\\quad \\quad w = w - \\eta g $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892abe6f",
   "metadata": {},
   "source": [
    "**Implement a `LogisticRegression` class with five methods: `train`, `predict`, `calculate_loss`, `calculate_gradient`, and `calculate_sigmoid`.** You may NOT use sklearn for this implementation. It is highly recommended that you vectorize your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaba976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:14:07.008030Z",
     "start_time": "2021-10-07T03:14:06.988485Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    '''\n",
    "    Logistic regression model with L2 regularization.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    alpha: regularization parameter\n",
    "    t: number of epochs to run gradient descent\n",
    "    eta: learning rate for gradient descent\n",
    "    w: (n x 1) weight vector\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, alpha=0, t=100, eta=1e-3):\n",
    "        self.alpha = alpha\n",
    "        self.t = t\n",
    "        self.eta = eta\n",
    "        self.w = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''Trains logistic regression model using gradient descent \n",
    "        (sets w to its optimal value).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        losses: (t x 1) vector of losses at each epoch of gradient descent\n",
    "        '''\n",
    "        ### Your code here\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''Predicts on X using trained model. Make sure to threshold \n",
    "        the predicted probability to return a 0 or 1 prediction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: (m x 1) 0/1 prediction vector\n",
    "        '''\n",
    "        ### Your code here\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        '''Calculates the logistic regression loss using X, y, w, \n",
    "        and alpha. Useful as a helper function for train().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss: (scalar) logistic regression loss\n",
    "        '''\n",
    "        ### Your code here\n",
    "    \n",
    "    def calculate_gradient(self, X, y):\n",
    "        '''Calculates the gradient of the logistic regression loss \n",
    "        using X, y, w, and alpha. Useful as a helper function \n",
    "        for train().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gradient: (n x 1) gradient vector for logistic regression loss\n",
    "        '''\n",
    "        ### Your code here\n",
    "    \n",
    "    def calculate_sigmoid(self, x):\n",
    "        '''Calculates the sigmoid function on each element in vector x. \n",
    "        Useful as a helper function for predict(), calculate_loss(), \n",
    "        and calculate_gradient().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: (m x 1) vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        sigmoid_x: (m x 1) vector of sigmoid on each element in x\n",
    "        '''\n",
    "        ### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f00312",
   "metadata": {},
   "source": [
    "### 2.4 Train, Evaluate, and Interpret Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4649613c",
   "metadata": {},
   "source": [
    "**Using your implementation above, train a logistic regression model (alpha=0, t=100, eta=1e-3) on the breast cancer training data. Plot the training loss over epochs. Make sure to label your axes.** You should see the loss decreasing and start to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb626d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:14:56.207356Z",
     "start_time": "2021-10-07T03:14:55.990691Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ef608",
   "metadata": {},
   "source": [
    "**Use your trained model to make predictions and report the accuracy on the training, validation, and test sets. Report the first 5 predictions on the test set, along with the actual labels.** Your accuracies should be much higher than the baseline accuracy we found in Section 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d698a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:15:53.655580Z",
     "start_time": "2021-10-07T03:15:53.644909Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53d107",
   "metadata": {},
   "source": [
    "**Interpret your trained model using a bar chart of the model weights.** Make sure to label the bars (x-axis) and don't forget the bias term! Use lecture 3, slide 15 as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d19493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:17:00.228116Z",
     "start_time": "2021-10-07T03:16:59.817413Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f38201",
   "metadata": {},
   "source": [
    "**According to your model, which features are the greatest contributors to the diagnosis?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bf476",
   "metadata": {},
   "source": [
    "### 2.5 Tune Regularization Parameter  ùõº"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc475d",
   "metadata": {},
   "source": [
    "Now, we will observe the effect of tuning the regularization parameter $\\alpha$ on the learned weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606ac5b",
   "metadata": {},
   "source": [
    "**Sweep out values for $\\alpha$ using `alphas = np.logspace(-2, 2, 100)`. For each $\\alpha$ value, train a logistic regression model and record its weights. Plot the weights for each feature as a function of $\\alpha$ on a single figure. Make sure to label the axes.** You should have 31 curves (one for each feature) in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a50048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:19:24.798491Z",
     "start_time": "2021-10-07T03:19:23.666668Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b880a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T02:27:09.132072Z",
     "start_time": "2021-10-07T02:27:09.126082Z"
    }
   },
   "source": [
    "**Describe the effect of the regularization parameter $\\alpha$ on the weights of your model. Please explain in terms of model complexity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0981942",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here\n",
    "\n",
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd1c8e",
   "metadata": {},
   "source": [
    "## Part 3: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2702a",
   "metadata": {},
   "source": [
    "You are allowed to use sklearn or any ML library in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9856b9a",
   "metadata": {},
   "source": [
    "### 3.1 Train Primal SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e233419",
   "metadata": {},
   "source": [
    "**Train a primal SVM (with default parameters) on the breast cancer training data. Make predictions and report the accuracy on the training, validation, and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea85df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:22:07.877954Z",
     "start_time": "2021-10-07T03:22:07.846738Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28716fa5",
   "metadata": {},
   "source": [
    "### 3.2 Train Dual SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb66a5",
   "metadata": {},
   "source": [
    "**Train a dual SVM (with default parameters) on the breast cancer training data. Make predictions and report the accuracy on the training, validation, and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba84cfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:22:14.144794Z",
     "start_time": "2021-10-07T03:22:14.132439Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bd677",
   "metadata": {},
   "source": [
    "### 3.3 Number of Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd4bd9",
   "metadata": {},
   "source": [
    "**Find the number of support vectors in your SVM model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8d2d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:22:17.001597Z",
     "start_time": "2021-10-07T03:22:16.995313Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e3c75",
   "metadata": {},
   "source": [
    "### 3.4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0961329",
   "metadata": {},
   "source": [
    "**Improve the SVM model (by hyperparameter tuning, feature selection, or using a non-linear SVM) to get better test performance than the dual SVM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57991f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T03:22:33.428903Z",
     "start_time": "2021-10-07T03:22:33.402969Z"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
